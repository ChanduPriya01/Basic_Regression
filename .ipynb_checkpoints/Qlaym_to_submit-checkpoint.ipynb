{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the basic packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "data_train = pd.read_csv(r\"Input path\\dataset_known.csv\")\n",
    "data_test = pd.read_csv(r\"Input path\\dataset_unknown.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var001</th>\n",
       "      <th>var002</th>\n",
       "      <th>var003</th>\n",
       "      <th>var005</th>\n",
       "      <th>var006</th>\n",
       "      <th>var007</th>\n",
       "      <th>var008</th>\n",
       "      <th>var009</th>\n",
       "      <th>var010</th>\n",
       "      <th>var011</th>\n",
       "      <th>...</th>\n",
       "      <th>var190</th>\n",
       "      <th>var192</th>\n",
       "      <th>var193</th>\n",
       "      <th>var194</th>\n",
       "      <th>var195</th>\n",
       "      <th>var196</th>\n",
       "      <th>var197</th>\n",
       "      <th>var198</th>\n",
       "      <th>var199</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1981.000000</td>\n",
       "      <td>1983.000000</td>\n",
       "      <td>1977.000000</td>\n",
       "      <td>1980.000000</td>\n",
       "      <td>1978.000000</td>\n",
       "      <td>1988.000000</td>\n",
       "      <td>1978.000000</td>\n",
       "      <td>1980.000000</td>\n",
       "      <td>1980.000000</td>\n",
       "      <td>1981.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1982.000000</td>\n",
       "      <td>1983.000000</td>\n",
       "      <td>1988.000000</td>\n",
       "      <td>1988.000000</td>\n",
       "      <td>1981.000000</td>\n",
       "      <td>1974.000000</td>\n",
       "      <td>1976.000000</td>\n",
       "      <td>1977.000000</td>\n",
       "      <td>1977.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>-2.267466</td>\n",
       "      <td>1.836243</td>\n",
       "      <td>5.496211</td>\n",
       "      <td>8.982192</td>\n",
       "      <td>4.342836</td>\n",
       "      <td>7.713581</td>\n",
       "      <td>1.084328</td>\n",
       "      <td>-1.104167</td>\n",
       "      <td>4.924818</td>\n",
       "      <td>-3.826078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502018</td>\n",
       "      <td>-7.069687</td>\n",
       "      <td>7.480004</td>\n",
       "      <td>12.360397</td>\n",
       "      <td>-6.798627</td>\n",
       "      <td>13.063166</td>\n",
       "      <td>9.049403</td>\n",
       "      <td>3.934717</td>\n",
       "      <td>0.087506</td>\n",
       "      <td>-0.002020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>2.941831</td>\n",
       "      <td>0.886738</td>\n",
       "      <td>0.188605</td>\n",
       "      <td>0.729381</td>\n",
       "      <td>3.085979</td>\n",
       "      <td>0.949286</td>\n",
       "      <td>3.020064</td>\n",
       "      <td>1.360280</td>\n",
       "      <td>0.132972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500122</td>\n",
       "      <td>2.976990</td>\n",
       "      <td>6.544741</td>\n",
       "      <td>4.978338</td>\n",
       "      <td>3.356861</td>\n",
       "      <td>3.898883</td>\n",
       "      <td>3.172295</td>\n",
       "      <td>3.657636</td>\n",
       "      <td>0.282647</td>\n",
       "      <td>1.003781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-2.460000</td>\n",
       "      <td>-7.510000</td>\n",
       "      <td>3.970000</td>\n",
       "      <td>8.420000</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>-0.528000</td>\n",
       "      <td>-11.500000</td>\n",
       "      <td>1.180000</td>\n",
       "      <td>-4.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-12.200000</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>3.430000</td>\n",
       "      <td>-17.600000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>3.540000</td>\n",
       "      <td>-2.410000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>-2.320000</td>\n",
       "      <td>-0.141000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>8.850000</td>\n",
       "      <td>3.860000</td>\n",
       "      <td>5.067500</td>\n",
       "      <td>0.238250</td>\n",
       "      <td>-3.122500</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>-3.920000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.545000</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>8.115000</td>\n",
       "      <td>-9.070000</td>\n",
       "      <td>9.832500</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>0.833000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.671250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>-2.260000</td>\n",
       "      <td>1.840000</td>\n",
       "      <td>5.510000</td>\n",
       "      <td>8.980000</td>\n",
       "      <td>4.340000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>-1.115000</td>\n",
       "      <td>4.955000</td>\n",
       "      <td>-3.820000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-6.990000</td>\n",
       "      <td>7.350000</td>\n",
       "      <td>12.550000</td>\n",
       "      <td>-6.840000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.135000</td>\n",
       "      <td>3.870000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>-2.210000</td>\n",
       "      <td>3.865000</td>\n",
       "      <td>6.270000</td>\n",
       "      <td>9.110000</td>\n",
       "      <td>4.830000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>1.012500</td>\n",
       "      <td>5.782500</td>\n",
       "      <td>-3.730000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.535000</td>\n",
       "      <td>11.900000</td>\n",
       "      <td>16.600000</td>\n",
       "      <td>-4.580000</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>7.220000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>-2.020000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.570000</td>\n",
       "      <td>6.930000</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>9.130000</td>\n",
       "      <td>-3.390000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.960000</td>\n",
       "      <td>29.900000</td>\n",
       "      <td>20.800000</td>\n",
       "      <td>3.540000</td>\n",
       "      <td>19.800000</td>\n",
       "      <td>14.300000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.170000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            var001       var002       var003       var005       var006  \\\n",
       "count  1981.000000  1983.000000  1977.000000  1980.000000  1978.000000   \n",
       "mean     -2.267466     1.836243     5.496211     8.982192     4.342836   \n",
       "std       0.080069     2.941831     0.886738     0.188605     0.729381   \n",
       "min      -2.460000    -7.510000     3.970000     8.420000     1.570000   \n",
       "25%      -2.320000    -0.141000     4.720000     8.850000     3.860000   \n",
       "50%      -2.260000     1.840000     5.510000     8.980000     4.340000   \n",
       "75%      -2.210000     3.865000     6.270000     9.110000     4.830000   \n",
       "max      -2.020000    11.200000     7.000000     9.570000     6.930000   \n",
       "\n",
       "            var007       var008       var009       var010       var011  ...  \\\n",
       "count  1988.000000  1978.000000  1980.000000  1980.000000  1981.000000  ...   \n",
       "mean      7.713581     1.084328    -1.104167     4.924818    -3.826078  ...   \n",
       "std       3.085979     0.949286     3.020064     1.360280     0.132972  ...   \n",
       "min       2.400000    -0.528000   -11.500000     1.180000    -4.200000  ...   \n",
       "25%       5.067500     0.238250    -3.122500     4.050000    -3.920000  ...   \n",
       "50%       7.750000     1.090000    -1.115000     4.955000    -3.820000  ...   \n",
       "75%      10.300000     1.900000     1.012500     5.782500    -3.730000  ...   \n",
       "max      13.100000     2.740000    11.500000     9.130000    -3.390000  ...   \n",
       "\n",
       "            var190       var192       var193       var194       var195  \\\n",
       "count  1982.000000  1983.000000  1988.000000  1988.000000  1981.000000   \n",
       "mean      0.502018    -7.069687     7.480004    12.360397    -6.798627   \n",
       "std       0.500122     2.976990     6.544741     4.978338     3.356861   \n",
       "min       0.000000   -12.200000   -15.000000     3.430000   -17.600000   \n",
       "25%       0.000000    -9.545000     3.220000     8.115000    -9.070000   \n",
       "50%       1.000000    -6.990000     7.350000    12.550000    -6.840000   \n",
       "75%       1.000000    -4.535000    11.900000    16.600000    -4.580000   \n",
       "max       1.000000    -1.960000    29.900000    20.800000     3.540000   \n",
       "\n",
       "            var196       var197       var198       var199       target  \n",
       "count  1974.000000  1976.000000  1977.000000  1977.000000  2000.000000  \n",
       "mean     13.063166     9.049403     3.934717     0.087506    -0.002020  \n",
       "std       3.898883     3.172295     3.657636     0.282647     1.003781  \n",
       "min       6.300000     3.540000    -2.410000     0.000000    -2.100000  \n",
       "25%       9.832500     6.250000     0.833000     0.000000    -0.671250  \n",
       "50%      13.000000     9.135000     3.870000     0.000000    -0.131500  \n",
       "75%      16.400000    11.800000     7.220000     0.000000     0.470250  \n",
       "max      19.800000    14.300000    10.200000     1.000000     4.170000  \n",
       "\n",
       "[8 rows x 181 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x148c16b6308>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x=data_train['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, the Outliers can be seen in the figure above. Lets get the outlier values from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_outliers(dataset):\n",
    "    q1, q3= np.percentile(dataset,[25,75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - (1.5 * iqr)\n",
    "    upper_bound = q3 + (1.5 * iqr)\n",
    "    outliers = pd. concat([pd.Series(dataset[dataset<lower_bound], index = (dataset[dataset<lower_bound]).index) , pd.Series(dataset[dataset>upper_bound], index=(dataset[dataset>upper_bound]).index)], axis = 0)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35      2.89\n",
      "99      2.82\n",
      "118     3.03\n",
      "153     2.66\n",
      "160     2.49\n",
      "        ... \n",
      "1963    3.07\n",
      "1988    3.05\n",
      "1993    2.31\n",
      "1995    2.95\n",
      "1998    2.62\n",
      "Name: target, Length: 95, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "outliers = Get_outliers(data_train['target'])\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the Outliers from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1905\n"
     ]
    }
   ],
   "source": [
    "data_train = data_train.drop(outliers.index, axis=0)\n",
    "print(len(data_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the missing values\n",
    "def Check_MissingValues(data):\n",
    "    for i in data.columns:\n",
    "        print(\"{} -{}\".format(i, data[i].isna().sum().sum()))\n",
    "    print(\"Data has a total missing values of\", data_train.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var001 -18\n",
      "var002 -17\n",
      "var003 -23\n",
      "var004 -0\n",
      "var005 -19\n",
      "var006 -20\n",
      "var007 -12\n",
      "var008 -20\n",
      "var009 -19\n",
      "var010 -19\n",
      "var011 -18\n",
      "var012 -19\n",
      "var013 -11\n",
      "var014 -19\n",
      "var015 -15\n",
      "var016 -23\n",
      "var017 -16\n",
      "var018 -21\n",
      "var019 -21\n",
      "var020 -17\n",
      "var021 -18\n",
      "var022 -0\n",
      "var023 -15\n",
      "var024 -17\n",
      "var025 -14\n",
      "var026 -11\n",
      "var027 -18\n",
      "var028 -20\n",
      "var029 -21\n",
      "var030 -26\n",
      "var031 -16\n",
      "var032 -15\n",
      "var033 -21\n",
      "var034 -27\n",
      "var035 -20\n",
      "var036 -0\n",
      "var037 -18\n",
      "var038 -21\n",
      "var039 -23\n",
      "var040 -15\n",
      "var041 -0\n",
      "var042 -18\n",
      "var043 -27\n",
      "var044 -23\n",
      "var045 -14\n",
      "var046 -25\n",
      "var047 -23\n",
      "var048 -27\n",
      "var049 -20\n",
      "var050 -19\n",
      "var051 -23\n",
      "var052 -22\n",
      "var053 -19\n",
      "var054 -0\n",
      "var055 -28\n",
      "var056 -17\n",
      "var057 -19\n",
      "var058 -20\n",
      "var059 -0\n",
      "var060 -15\n",
      "var061 -14\n",
      "var062 -17\n",
      "var063 -11\n",
      "var064 -10\n",
      "var065 -15\n",
      "var066 -24\n",
      "var067 -20\n",
      "var068 -17\n",
      "var069 -22\n",
      "var070 -26\n",
      "var071 -17\n",
      "var072 -11\n",
      "var073 -17\n",
      "var074 -19\n",
      "var075 -14\n",
      "var076 -0\n",
      "var077 -19\n",
      "var078 -19\n",
      "var079 -16\n",
      "var080 -16\n",
      "var081 -27\n",
      "var082 -18\n",
      "var083 -20\n",
      "var084 -19\n",
      "var085 -19\n",
      "var086 -18\n",
      "var087 -16\n",
      "var088 -22\n",
      "var089 -0\n",
      "var090 -15\n",
      "var091 -18\n",
      "var092 -20\n",
      "var093 -18\n",
      "var094 -16\n",
      "var095 -8\n",
      "var096 -20\n",
      "var097 -16\n",
      "var098 -17\n",
      "var099 -14\n",
      "var100 -18\n",
      "var101 -19\n",
      "var102 -23\n",
      "var103 -19\n",
      "var104 -15\n",
      "var105 -18\n",
      "var106 -13\n",
      "var107 -17\n",
      "var108 -20\n",
      "var109 -16\n",
      "var110 -18\n",
      "var111 -0\n",
      "var112 -0\n",
      "var113 -23\n",
      "var114 -19\n",
      "var115 -19\n",
      "var116 -22\n",
      "var117 -14\n",
      "var118 -14\n",
      "var119 -16\n",
      "var120 -14\n",
      "var121 -19\n",
      "var122 -17\n",
      "var123 -30\n",
      "var124 -18\n",
      "var125 -15\n",
      "var126 -20\n",
      "var127 -13\n",
      "var128 -16\n",
      "var129 -19\n",
      "var130 -25\n",
      "var131 -25\n",
      "var132 -15\n",
      "var133 -23\n",
      "var134 -17\n",
      "var135 -30\n",
      "var136 -20\n",
      "var137 -16\n",
      "var138 -19\n",
      "var139 -27\n",
      "var140 -20\n",
      "var141 -18\n",
      "var142 -18\n",
      "var143 -20\n",
      "var144 -20\n",
      "var145 -31\n",
      "var146 -24\n",
      "var147 -21\n",
      "var148 -14\n",
      "var149 -17\n",
      "var150 -22\n",
      "var151 -0\n",
      "var152 -0\n",
      "var153 -26\n",
      "var154 -17\n",
      "var155 -20\n",
      "var156 -24\n",
      "var157 -21\n",
      "var158 -0\n",
      "var159 -20\n",
      "var160 -24\n",
      "var161 -17\n",
      "var162 -0\n",
      "var163 -21\n",
      "var164 -19\n",
      "var165 -19\n",
      "var166 -23\n",
      "var167 -23\n",
      "var168 -23\n",
      "var169 -14\n",
      "var170 -15\n",
      "var171 -21\n",
      "var172 -15\n",
      "var173 -18\n",
      "var174 -23\n",
      "var175 -18\n",
      "var176 -0\n",
      "var177 -21\n",
      "var178 -19\n",
      "var179 -20\n",
      "var180 -15\n",
      "var181 -24\n",
      "var182 -12\n",
      "var183 -26\n",
      "var184 -0\n",
      "var185 -0\n",
      "var186 -16\n",
      "var187 -18\n",
      "var188 -0\n",
      "var189 -25\n",
      "var190 -18\n",
      "var191 -0\n",
      "var192 -17\n",
      "var193 -12\n",
      "var194 -10\n",
      "var195 -18\n",
      "var196 -23\n",
      "var197 -23\n",
      "var198 -23\n",
      "var199 -23\n",
      "var200 -0\n",
      "target -0\n",
      "Data has a total missing values of 3417\n"
     ]
    }
   ],
   "source": [
    "Check_MissingValues(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Drop_MissingValues(data):\n",
    "    print(\"Length of the data is\", len(data))\n",
    "    data_no_missing_values = data.dropna(axis=0)\n",
    "    print(\"Length of the data after removing the missing values is\", len(data_no_missing_values))\n",
    "    return data_no_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data is 1905\n",
      "Length of the data after removing the missing values is 308\n"
     ]
    }
   ],
   "source": [
    "data_train_no_MV = Drop_MissingValues(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a lot of data is lost, the missing values in Numerical Columns are imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the numerical and Categorical features\n",
    "def Separate_Numeric_Categorical_Features(data):\n",
    "    Numerical_columns = []\n",
    "    Categorical_columns = []\n",
    "    for i in data.columns:\n",
    "        if data[i].dtype != \"object\" :\n",
    "            Numerical_columns.append(i)\n",
    "        else:\n",
    "            Categorical_columns.append(i)\n",
    "    return Numerical_columns, Categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numerical_columns, Categorical_columns = Separate_Numeric_Categorical_Features(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing the missing values of numerical columns with the mean of that column\n",
    "for i in Numerical_columns:\n",
    "    data_train[i].fillna(data_train[i].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data is 1905\n",
      "Length of the data after removing the missing values is 1905\n"
     ]
    }
   ],
   "source": [
    "data_train = Drop_MissingValues(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missed values now. The Categorical Columns are checked for values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_UniqueValues(data, columns):\n",
    "    for i in columns:\n",
    "        print(data[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b' 'a' 'c' '<undefined>']\n",
      "['p4' 'p2' 'p5' 'p1' 'p3' '<undefined>']\n",
      "['a' 'b' 'c' '<undefined>']\n",
      "['c' '<undefined>' 'a' 'b']\n",
      "['c' 'a' 'b' '<undefined>']\n",
      "['p4' 'p1' 'p2' 'p5' 'p3' '<undefined>']\n",
      "['p3' 'p2' 'p5' 'p1' 'p4' '<undefined>']\n",
      "['b' 'a' 'c' '<undefined>']\n",
      "['p1' 'p4' 'p5' 'p3' 'p2' '<undefined>']\n",
      "['p5' 'p2' 'p1' 'p4' 'p3' '<undefined>']\n",
      "['b' 'a' 'c' '<undefined>']\n",
      "['p4' 'p2' 'p1' 'p3' 'p5' '<undefined>']\n",
      "['p2' 'p3' 'p1' 'p5' 'p4' '<undefined>']\n",
      "['a' '<undefined>' 'c' 'b']\n",
      "['c' 'b' 'a' '<undefined>']\n",
      "['p4' 'p1' 'p2' 'p5' 'p3' '<undefined>']\n",
      "['p2' 'p1' 'p4' 'p5' 'p3' '<undefined>']\n",
      "['p3' 'p5' 'p4' 'p2' 'p1' '<undefined>']\n",
      "['a' 'c' 'b' '<undefined>']\n",
      "['b' 'a' 'c' '<undefined>']\n"
     ]
    }
   ],
   "source": [
    "Check_UniqueValues(data_train, Categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"< undefined >\" can be replaced/Removed as it might affect the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train = data_train.replace(\"<undefined>\", np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data is 1905\n",
      "Length of the data after removing the missing values is 1542\n"
     ]
    }
   ],
   "source": [
    "data_train = Drop_MissingValues(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, 80% of data is retained. Let the undefined values be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numerical_columns.remove('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the numerical values to a scale of (0,1)\n",
    "from sklearn import preprocessing\n",
    "def Scaling(data, Numerical_columns):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler() \n",
    "    for i in Numerical_columns:\n",
    "        data[i] = min_max_scaler.fit_transform(np.array(data[i]).reshape(-1, 1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled = Scaling(data_train, Numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot coding the categorical columns\n",
    "def Get_OnehotEncoding(data, Categorical_columns):\n",
    "    data_input_one_hot = data\n",
    "    for i in Categorical_columns:        \n",
    "        # One column from One hot encodings is removed to avoid the dummy value trap\n",
    "        dummies = pd.get_dummies(data_input_one_hot[i], drop_first=True, prefix=i)\n",
    "        data_input_one_hot = pd.concat([data_input_one_hot, dummies], axis=1)\n",
    "        data_input_one_hot.drop([i], axis=1, inplace=True)\n",
    "    return data_input_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_one_hot = Get_OnehotEncoding(data_train_scaled, Categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var001 -5\n",
      "var002 -5\n",
      "var003 -5\n",
      "var004 -0\n",
      "var005 -6\n",
      "var006 -4\n",
      "var007 -4\n",
      "var008 -6\n",
      "var009 -7\n",
      "var010 -5\n",
      "var011 -7\n",
      "var012 -5\n",
      "var013 -4\n",
      "var014 -5\n",
      "var015 -3\n",
      "var016 -8\n",
      "var017 -3\n",
      "var018 -3\n",
      "var019 -10\n",
      "var020 -4\n",
      "var021 -7\n",
      "var022 -0\n",
      "var023 -6\n",
      "var024 -6\n",
      "var025 -6\n",
      "var026 -3\n",
      "var027 -6\n",
      "var028 -6\n",
      "var029 -8\n",
      "var030 -6\n",
      "var031 -4\n",
      "var032 -4\n",
      "var033 -4\n",
      "var034 -5\n",
      "var035 -9\n",
      "var036 -0\n",
      "var037 -3\n",
      "var038 -6\n",
      "var039 -6\n",
      "var040 -4\n",
      "var041 -0\n",
      "var042 -5\n",
      "var043 -2\n",
      "var044 -3\n",
      "var045 -7\n",
      "var046 -6\n",
      "var047 -4\n",
      "var048 -2\n",
      "var049 -8\n",
      "var050 -3\n",
      "var051 -3\n",
      "var052 -3\n",
      "var053 -8\n",
      "var054 -0\n",
      "var055 -6\n",
      "var056 -4\n",
      "var057 -5\n",
      "var058 -8\n",
      "var059 -0\n",
      "var060 -2\n",
      "var061 -7\n",
      "var062 -4\n",
      "var063 -4\n",
      "var064 -7\n",
      "var065 -7\n",
      "var066 -4\n",
      "var067 -4\n",
      "var068 -5\n",
      "var069 -3\n",
      "var070 -5\n",
      "var071 -3\n",
      "var072 -2\n",
      "var073 -4\n",
      "var074 -6\n",
      "var075 -4\n",
      "var076 -0\n",
      "var077 -7\n",
      "var078 -8\n",
      "var079 -7\n",
      "var080 -7\n",
      "var081 -3\n",
      "var082 -8\n",
      "var083 -3\n",
      "var084 -7\n",
      "var085 -5\n",
      "var086 -2\n",
      "var087 -5\n",
      "var088 -10\n",
      "var089 -0\n",
      "var090 -11\n",
      "var091 -4\n",
      "var092 -4\n",
      "var093 -7\n",
      "var094 -5\n",
      "var095 -3\n",
      "var096 -10\n",
      "var097 -2\n",
      "var098 -7\n",
      "var099 -7\n",
      "var100 -5\n",
      "var101 -6\n",
      "var102 -2\n",
      "var103 -5\n",
      "var104 -9\n",
      "var105 -2\n",
      "var106 -3\n",
      "var107 -7\n",
      "var108 -7\n",
      "var109 -5\n",
      "var110 -7\n",
      "var111 -0\n",
      "var112 -0\n",
      "var113 -5\n",
      "var114 -6\n",
      "var115 -5\n",
      "var116 -4\n",
      "var117 -4\n",
      "var118 -6\n",
      "var119 -7\n",
      "var120 -2\n",
      "var121 -6\n",
      "var122 -6\n",
      "var123 -2\n",
      "var124 -3\n",
      "var125 -6\n",
      "var126 -6\n",
      "var127 -6\n",
      "var128 -4\n",
      "var129 -5\n",
      "var130 -2\n",
      "var131 -4\n",
      "var132 -8\n",
      "var133 -4\n",
      "var134 -2\n",
      "var135 -2\n",
      "var136 -7\n",
      "var137 -1\n",
      "var138 -4\n",
      "var139 -5\n",
      "var140 -6\n",
      "var141 -10\n",
      "var142 -4\n",
      "var143 -2\n",
      "var144 -7\n",
      "var145 -5\n",
      "var146 -2\n",
      "var147 -3\n",
      "var148 -2\n",
      "var149 -8\n",
      "var150 -9\n",
      "var151 -0\n",
      "var152 -0\n",
      "var153 -2\n",
      "var154 -2\n",
      "var155 -7\n",
      "var156 -5\n",
      "var157 -5\n",
      "var158 -0\n",
      "var159 -7\n",
      "var160 -5\n",
      "var161 -4\n",
      "var162 -0\n",
      "var163 -4\n",
      "var164 -5\n",
      "var165 -4\n",
      "var166 -3\n",
      "var167 -4\n",
      "var168 -6\n",
      "var169 -4\n",
      "var170 -7\n",
      "var171 -5\n",
      "var172 -5\n",
      "var173 -7\n",
      "var174 -3\n",
      "var175 -1\n",
      "var176 -0\n",
      "var177 -6\n",
      "var178 -6\n",
      "var179 -5\n",
      "var180 -5\n",
      "var181 -4\n",
      "var182 -4\n",
      "var183 -4\n",
      "var184 -0\n",
      "var185 -0\n",
      "var186 -3\n",
      "var187 -3\n",
      "var188 -0\n",
      "var189 -6\n",
      "var190 -3\n",
      "var191 -0\n",
      "var192 -8\n",
      "var193 -2\n",
      "var194 -3\n",
      "var195 -4\n",
      "var196 -4\n",
      "var197 -11\n",
      "var198 -3\n",
      "var199 -4\n",
      "var200 -0\n",
      "Data has a total missing values of 0\n"
     ]
    }
   ],
   "source": [
    "Check_MissingValues(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing the numeric columns\n",
    "for i in Numerical_columns:\n",
    "    data_test[i].fillna(data_test[i].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b' 'a' 'c' '<undefined>']\n",
      "['p3' 'p5' 'p2' 'p4' 'p1' '<undefined>']\n",
      "['b' 'a' 'c' '<undefined>']\n",
      "['b' 'c' 'a' '<undefined>']\n",
      "['<undefined>' 'b' 'c' 'a']\n",
      "['p1' 'p3' 'p2' 'p4' 'p5' '<undefined>']\n",
      "['p4' 'p2' 'p3' 'p5' 'p1' '<undefined>']\n",
      "['b' 'c' 'a' '<undefined>']\n",
      "['p2' 'p3' 'p4' 'p5' 'p1' '<undefined>']\n",
      "['p2' 'p3' 'p5' 'p1' 'p4' '<undefined>']\n",
      "['c' 'a' 'b' '<undefined>']\n",
      "['p3' 'p2' 'p4' 'p5' 'p1' '<undefined>']\n",
      "['p4' 'p2' 'p5' 'p3' 'p1' '<undefined>']\n",
      "['b' 'a' 'c' '<undefined>']\n",
      "['b' 'a' 'c' '<undefined>']\n",
      "['p3' 'p2' 'p1' 'p5' 'p4' '<undefined>']\n",
      "['p3' 'p5' 'p4' 'p1' 'p2' '<undefined>']\n",
      "['p1' 'p2' 'p5' 'p3' 'p4' '<undefined>']\n",
      "['c' 'b' 'a' '<undefined>']\n",
      "['b' 'c' 'a' '<undefined>']\n"
     ]
    }
   ],
   "source": [
    "Check_UniqueValues(data_test, Categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.replace(\"<undefined>\", np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data is 500\n",
      "Length of the data after removing the missing values is 418\n"
     ]
    }
   ],
   "source": [
    "data_test = Drop_MissingValues(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_scaled = Scaling(data_test, Numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_one_hot = Get_OnehotEncoding(data_test_scaled, Categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = data_train_one_hot['target']\n",
    "input_train = data_train_one_hot.drop(['target'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, val_X, train_y, val_y = train_test_split(input_train, target_train, test_size=0.35,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-1 : Randomforest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods are generally stronger because they use more than one model for a prediction. To start with a model, as a personal choice Random forest is chosen. Other boosting and bagging methods from ensemble could also be used for the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Training Score: 0.92 \n",
      "OOB Score: 0.44 \n",
      "R^2 Validation Score: 0.44\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 100,\n",
    "                           n_jobs = -1,\n",
    "                           oob_score = True,\n",
    "                           bootstrap = True,\n",
    "                           random_state = 42)\n",
    "rf.fit(train_X, train_y)\n",
    "print('R^2 Training Score: {:.2f} \\nOOB Score: {:.2f} \\nR^2 Validation Score: {:.2f}'.format(rf.score(train_X, train_y), \n",
    "                                                                                             rf.oob_score_,\n",
    "                                                                                             rf.score(val_X, val_y)))\n",
    "features = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(train_X.columns, rf.feature_importances_):\n",
    "    features[feature] = importance    \n",
    "importances = pd.DataFrame.from_dict(features, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "importances = importances.sort_values(by='Gini-importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Model with least important features eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['var046', 'var087', 'var079', 'var058', 'var026', 'var090', 'var049',\n",
      "       'var084', 'var160', 'var081',\n",
      "       ...\n",
      "       'var004_c', 'var189', 'var024', 'var008', 'var136', 'var145', 'var153',\n",
      "       'var009', 'var170', 'var045'],\n",
      "      dtype='object', length=154)\n"
     ]
    }
   ],
   "source": [
    "higher_importances1 = importances[importances>0.001]\n",
    "higher_importances1.dropna(axis=0, inplace=True)\n",
    "print(higher_importances1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input1 = input_train[higher_importances1.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(data_input1, target_train, test_size=0.35,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Training Score: 0.92 \n",
      "OOB Score: 0.45 \n",
      "R^2 Validation Score: 0.44\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 100,\n",
    "                           n_jobs = -1,\n",
    "                           oob_score = True,\n",
    "                           bootstrap = True,\n",
    "                           random_state = 42)\n",
    "rf.fit(train_X, train_y)\n",
    "print('R^2 Training Score: {:.2f} \\nOOB Score: {:.2f} \\nR^2 Validation Score: {:.2f}'.format(rf.score(train_X, train_y), \n",
    "                                                                                             rf.oob_score_,\n",
    "                                                                                             rf.score(val_X, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Model with Few input features of good importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "higher_importances2 = importances[importances>0.01]\n",
    "higher_importances2.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input2 = input_train[higher_importances2.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(data_input2, target_train, test_size=0.35,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Training Score: 0.93 \n",
      "OOB Score: 0.53 \n",
      "R^2 Validation Score: 0.51\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 100,\n",
    "                           n_jobs = -1,\n",
    "                           oob_score = True,\n",
    "                           bootstrap = True,\n",
    "                           random_state = 42)\n",
    "rf.fit(train_X, train_y)\n",
    "print('R^2 Training Score: {:.2f} \\nOOB Score: {:.2f} \\nR^2 Validation Score: {:.2f}'.format(rf.score(train_X, train_y), \n",
    "                                                                                             rf.oob_score_,\n",
    "                                                                                             rf.score(val_X, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Model with input features of higher importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "higher_importances3 = importances[importances>0.05]\n",
    "higher_importances3.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input3 = input_train[higher_importances3.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(data_input3, target_train, test_size=0.35,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Training Score: 0.90 \n",
      "OOB Score: 0.26 \n",
      "R^2 Validation Score: 0.21\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 100,\n",
    "                           n_jobs = -1,\n",
    "                           oob_score = True,\n",
    "                           bootstrap = True,\n",
    "                           random_state = 42)\n",
    "rf.fit(train_X, train_y)\n",
    "print('R^2 Training Score: {:.2f} \\nOOB Score: {:.2f} \\nR^2 Validation Score: {:.2f}'.format(rf.score(train_X, train_y), \n",
    "                                                                                             rf.oob_score_,\n",
    "                                                                                             rf.score(val_X, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-2: PCA + RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is an unsupervised machine learning technique mainly used for \"Dimensionality reduction\", by finding the correlation in between the Input features. Since, the number of input features are 200 (large), dimensionality reduction can give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 3 no.of Principal Components,\n",
      " R^2 Training Score: 0.84 \n",
      "OOB Score: -0.13 \n",
      "R^2 Validation Score: -0.14\n",
      " \n",
      "\n",
      "for 5 no.of Principal Components,\n",
      " R^2 Training Score: 0.85 \n",
      "OOB Score: -0.11 \n",
      "R^2 Validation Score: -0.09\n",
      " \n",
      "\n",
      "for 7 no.of Principal Components,\n",
      " R^2 Training Score: 0.85 \n",
      "OOB Score: -0.09 \n",
      "R^2 Validation Score: -0.10\n",
      " \n",
      "\n",
      "for 10 no.of Principal Components,\n",
      " R^2 Training Score: 0.86 \n",
      "OOB Score: -0.05 \n",
      "R^2 Validation Score: -0.06\n",
      " \n",
      "\n",
      "for 15 no.of Principal Components,\n",
      " R^2 Training Score: 0.86 \n",
      "OOB Score: -0.04 \n",
      "R^2 Validation Score: -0.03\n",
      " \n",
      "\n",
      "for 20 no.of Principal Components,\n",
      " R^2 Training Score: 0.86 \n",
      "OOB Score: -0.01 \n",
      "R^2 Validation Score: 0.04\n",
      " \n",
      "\n",
      "for 30 no.of Principal Components,\n",
      " R^2 Training Score: 0.86 \n",
      "OOB Score: -0.01 \n",
      "R^2 Validation Score: 0.03\n",
      " \n",
      "\n",
      "for 50 no.of Principal Components,\n",
      " R^2 Training Score: 0.87 \n",
      "OOB Score: 0.04 \n",
      "R^2 Validation Score: 0.07\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "for i in [3, 5 ,7, 10, 15, 20, 30, 50]:\n",
    "    pca = PCA(n_components=i) \n",
    "    data_pca_train = pd.DataFrame(pca.fit_transform(input_train))\n",
    "    train_X, val_X, train_y, val_y = train_test_split(data_pca_train, target_train, test_size=0.35,random_state=42)\n",
    "    rf = RandomForestRegressor(n_estimators = 100,\n",
    "                           n_jobs = -1,\n",
    "                           oob_score = True,\n",
    "                           bootstrap = True,\n",
    "                           random_state = 42)\n",
    "    rf.fit(train_X, train_y)\n",
    "    print('for {} no.of Principal Components,\\n R^2 Training Score: {:.2f} \\nOOB Score: {:.2f} \\nR^2 Validation Score: {:.2f}\\n \\n'.format(i, rf.score(train_X, train_y), \n",
    "                                                                                             rf.oob_score_,\n",
    "                                                                                             rf.score(val_X, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By considering the one-hot encoded variables, the performance of the model is very low. Therefore, the model is checked only with numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric = data_train_scaled.drop(Categorical_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking with multiple values of Principal Components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 3 no.of Principal Components,\n",
      " R^2 Training Score: 1.00 \n",
      "OOB Score: 0.97 \n",
      "R^2 Validation Score: 0.97\n",
      " \n",
      "\n",
      "for 5 no.of Principal Components,\n",
      " R^2 Training Score: 1.00 \n",
      "OOB Score: 0.97 \n",
      "R^2 Validation Score: 0.98\n",
      " \n",
      "\n",
      "for 7 no.of Principal Components,\n",
      " R^2 Training Score: 1.00 \n",
      "OOB Score: 0.97 \n",
      "R^2 Validation Score: 0.98\n",
      " \n",
      "\n",
      "for 10 no.of Principal Components,\n",
      " R^2 Training Score: 1.00 \n",
      "OOB Score: 0.98 \n",
      "R^2 Validation Score: 0.98\n",
      " \n",
      "\n",
      "for 15 no.of Principal Components,\n",
      " R^2 Training Score: 1.00 \n",
      "OOB Score: 0.98 \n",
      "R^2 Validation Score: 0.98\n",
      " \n",
      "\n",
      "for 20 no.of Principal Components,\n",
      " R^2 Training Score: 1.00 \n",
      "OOB Score: 0.98 \n",
      "R^2 Validation Score: 0.98\n",
      " \n",
      "\n",
      "for 30 no.of Principal Components,\n",
      " R^2 Training Score: 1.00 \n",
      "OOB Score: 0.98 \n",
      "R^2 Validation Score: 0.98\n",
      " \n",
      "\n",
      "for 50 no.of Principal Components,\n",
      " R^2 Training Score: 1.00 \n",
      "OOB Score: 0.98 \n",
      "R^2 Validation Score: 0.98\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [3, 5 ,7, 10, 15, 20, 30, 50]:\n",
    "    pca = PCA(n_components=i) \n",
    "    data_pca_train = pd.DataFrame(pca.fit_transform(data_numeric))\n",
    "    train_X, val_X, train_y, val_y = train_test_split(data_pca_train, target_train, test_size=0.35,random_state=42)\n",
    "    rf = RandomForestRegressor(n_estimators = 100,\n",
    "                           n_jobs = -1,\n",
    "                           oob_score = True,\n",
    "                           bootstrap = True,\n",
    "                           random_state = 42)\n",
    "    rf.fit(train_X, train_y)\n",
    "    print('for {} no.of Principal Components,\\n R^2 Training Score: {:.2f} \\nOOB Score: {:.2f} \\nR^2 Validation Score: {:.2f}\\n \\n'.format(i, rf.score(train_X, train_y), \n",
    "                                                                                             rf.oob_score_,\n",
    "                                                                                             rf.score(val_X, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed from 10, the scores did not change and got the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "                      oob_score=True, random_state=42, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=10) \n",
    "data_pca_train = pd.DataFrame(pca.fit_transform(data_numeric))\n",
    "train_X, val_X, train_y, val_y = train_test_split(data_pca_train, target_train, test_size=0.35,random_state=42)\n",
    "rf = RandomForestRegressor(n_estimators = 100,\n",
    "                           n_jobs = -1,\n",
    "                           oob_score = True,\n",
    "                           bootstrap = True,\n",
    "                           random_state = 42)\n",
    "rf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_numeric = data_test_scaled.drop(Categorical_columns, axis=1) \n",
    "data_pca_test = pd.DataFrame(pca.fit_transform(data_test_numeric))\n",
    "data_new = data_test\n",
    "for index, value in data_pca_test.iterrows():\n",
    "    pred_y = rf.predict(np.array(value).reshape(1, -1))\n",
    "    data_new.loc[index,\"target_value\"] = pred_y\n",
    "data_new.to_csv(r\"Input path\\unknown_data.csv\", index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since better results are obtained with Random forest regressor, the other methods are not checked."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
